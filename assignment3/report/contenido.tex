\textcolor{BrickRed}{\bf NOTA:}  Los ejercicios se encuentran repartidos en los archivos:
\begin{itemize}
	\item \textcolor{mediumblue}{ejercicio1.py}
	\item \textcolor{mediumblue}{ejercicio2.py}
\end{itemize}

% -----------------------------------------------------------------------------------------
\vspace{5mm}
{\color{lightgray} \hrule}
\begin{enumerate}
	\item Sea $Q$ una matriz unitaria aleatoria de $20 \times 20$ (eg. con $A$ una matriz de tamaño $20 \times 20$ aleatoria, calculen su descomposición $Q R$ ). Sean $\lambda_1>\lambda_2>\dots \geq \lambda_{20}=1>0$ y 
	\begin{equation} \label{eq:1}
		B=Q^{*} diag\left(\lambda_1, \lambda_2, \dots, \lambda_{20}\right) Q,
	\end{equation}
	\begin{equation} \label{eq:2}
		B_{\varepsilon}=Q^* diag\left(\lambda_1+\varepsilon_1, \lambda_2+\varepsilon_2, \ldots, \lambda_{20}+\varepsilon_{20}\right) Q,
	\end{equation}
	con $\varepsilon_i  \sim N(0, \sigma)$ y $ \sigma=0.025$.
	
	\begin{enumerate}
		\item Comparar la descomposición de Cholesky de $B$ y de $B_{\varepsilon}$ usando el algoritmo de la tarea 1. Considerar los casos cuando $B$ tiene un buen número de condición y un mal número de condición.
		\item Con el caso mal condicionado, comparar el resultado de su algoritmo con el del algoritmo de Cholesky de scipy.
		\item Medir el tiempo de ejecución de su algoritmo de Cholesky con el de scipy.
	\end{enumerate}
\end{enumerate}

\textcolor{BrickRed}{\it Respuesta:}

Se sabe que, dada una matriz $A$ aleatoria de $n\times n$, entonces su factorización $QR$ genera una matriz $Q$ unitaria de $n\times n$. En el archivo \textcolor{mediumblue}{ejercicio1.py}, se genera la matriz Q de esta forma en la función \textit{generar\_Bs()}, la cual toma como argumentos al eigenvalor más grande de $B:\lambda_1$, al más pequeño: $\lambda_{20}=1$, la desviación estándar del ruido: $\sigma = 0.025$ y la dimensión: $n=20$. Tal función regresa a las matrices $B$ y $B_\varepsilon$ como en \eqref{eq:1} y \eqref{eq:2}.

En clase se revisó que, para una matriz cuadrada $A$ de $n\times n$ con eigenvalores $\{\lambda_1 \geq \cdots \geq \lambda_n\}$, su norma $2$ cumple que $\left\|A\right\|_2 = \left|\lambda_1\right|$, y por lo tanto, el número de condición bajo la norma 2 está dado por
\begin{equation} \label{eq:3}
	\kappa_{2} \left(A\right) = \left|\frac{\lambda_1}{\lambda_n}\right|.
\end{equation}

Esto se usa en el archivo \textcolor{mediumblue}{ejercicio1.py} para generar $B$ bien o mal condicionada. Se usa un valor de $\lambda_1$ bastante grande para mal condicionar la matriz, y uno relativamente cerca de $\lambda_{20}=1$ para que esté bien condicionada.

{\color{blue}
\begin{enumerate}[label=(\alph*), start=1]
	\item  
\end{enumerate}}
Para el caso en que la matriz $B$ esté bien condicionada, se usó $\lambda_1=2$, i.e., $\kappa_{2} = 2$ y para cuando $B$ esté mal condicionada, se usó $\lambda_1 = 1,000,000,000$, entonces $\kappa_{2} = 1,000,000,000$.

Se ejecutó la función \textit{LU\_cholesky()} para ambas matrices $B$ y $B_{\varepsilon}$ para los casos bien y mal condicionados y los resultados fueron los esperados: para los $2$ casos, las matrices de Cholesky $R$ y $R_{\varepsilon}$ eran tal que $R^{T}R = B$ y $R_{\varepsilon}^{T}R_{\varepsilon} = B_{\varepsilon}$, esto se verificó revisando que la normas $\left\|R^{T}R - B\right\|_2$ y $\left\|R_{\varepsilon}^{T}R_{\varepsilon} - B_{\varepsilon}\right\|_2$ fueran casi cero ($<10^{-6}$). El resultado es esperado ya que, sin importar la magnitud del número de condición, se tiene que las matrices involucradas tienen eigenvalores positivos (dados por nosotros), implicando que sean definidas positivas y además, como $B$ y $B_\varepsilon$ son de la forma \eqref{eq:1} y \eqref{eq:2}, también son simétricas, así que la factorización de Cholesky siempre funcionará bien.

{\color{blue}
	\begin{enumerate}[label=(\alph*), start=2]
		\item  
\end{enumerate}}
Se usó la función \textit{linalg.cholesky()} de la librería \textit{Scipy} para calcular las matrices de Cholesky $R_{c}$ y $R_{\varepsilon_{c}}$ para el caso mal condicionado y se compararon con los resultados del inciso a). Se calcularon las normas $\left\|R_c - R\right\|_2$ y $\left\| R_{\varepsilon_{c}} - R_c \right\|_2$ y se verificó que fueran prácticamente cero ($<10^{-6}$)

{\color{blue}
	\begin{enumerate}[label=(\alph*), start=3]
		\item  
\end{enumerate}}
Se realizaron $1000$ simulaciones de $B$ y $B_\varepsilon$ como en \eqref{eq:1} y \eqref{eq:2} en las que se midieron los tiempos de ejecución de la función \textit{LU\_cholesky()} para comparar con los de \textit{linalg.cholesky()} de \textit{Scipy}. Tales resultados se promediaron y se obtuvieron los siguientes resultados (variaba en cada repetición, sin embargo, el comportamiento es el mismo):
\begin{itemize}
	\item Tiempo promedio LU\_cholesky($B$): $0.00025435566903979633$.
	\item Tiempo promedio scipy.cholesky($B$): $4.450875938346144^{-6}$.
	\item Tiempo promedio LU\_cholesky($B_{\varepsilon}$): $0.0002526311719484511$.
	\item Tiempo promedio scipy.cholesky($B_{\varepsilon}$): $3.492367010039743^{-6}$.
\end{itemize}
De aquí se puede notar que el método implementado en Scipy siempre es más rápido, pero con resultados igual de satisfactorios.

% -----------------------------------------------------------------------------------------
\vspace{5mm}
{\color{lightgray} \hrule}
\begin{enumerate} \setcounter{enumi}{1}
	\item Resolver el problema de mínimos cuadrados,
	\begin{equation} \label{eq:4}
		y=X \beta+\varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma)
	\end{equation}
	usando su implementación de la descomposición $Q R $; $\beta$ es de tamaño $n \times 1$ y $X$ de tamaño $n \times d$. Sean $d=5$, $n=20$, $\beta=(5,4,3,2,1)^{\prime}$ y $\sigma=0.12$.
	\begin{enumerate}
		\item Hacer $X$ con entradas aleatorias $U(0,1)$ y simular $y$. Encontrar $\hat{\beta}$ y compararlo con el obtenido $\hat{\beta}_p$ haciendo $X+\Delta X$, donde las entradas de $\Delta X$ son $N(0, \sigma=0.01)$. Comparar a su vez con $\hat{\beta}_c=\left((X+\Delta X)^{\prime}(X+\Delta X)\right)^{-1}(X+\Delta X)^{\prime} y$ usando el algoritmo genérico para invertir matrices scipy.linalg.inv.
		\item Lo mismo que el anterior pero con $X$ mal condicionada (i.e. con casi colinealidad).
	\end{enumerate}
\end{enumerate}

\textcolor{BrickRed}{\it Respuesta:}

{\color{blue}
	\begin{enumerate}[label=(\alph*), start=1]
		\item  
\end{enumerate}}
En el archivo \textcolor{mediumblue}{ejercicio2.py}, se implementa la función \textit{ajuste\_QR()}, la cual toma como argumentos a una matriz $X$ de $n\times d$, el vector $\beta$ de $d\times 1$ y la desviación estándar del ruido $\sigma=0.12$. En esta función se generan los valores de $y$ con ruido como en \eqref{eq:4} y luego se toma la factorización $QR$ de $X$ con ayuda de la función \textit{MODIFIED\_GRAM\_SCHMIDT()} de la tarea 2. Con esto, se resuelve el sistema
\begin{equation} \label{eq:5}
	R\beta = Q^{T}y
\end{equation}
con backward substitution ya que $R$ es una matriz triangular superior. Esta función da como salida a los coeficientes ajustados $\hat{\beta}$ que son solución a \eqref{eq:5}.

Se calculan $\hat{\beta}$ para $X$, $\hat{\beta}_p$ para $\tilde{X} = X+\Delta X$ y $\hat{\beta}_c = \left(\tilde{X}^{\prime}\tilde{X}\right)^{-1}\tilde{X}^{\prime} y$. Para una realización, se obtuvieron los siguientes resultados (se usó una semilla para reproducibilidad):
\begin{itemize}
	\item $\beta$ del modelo original: $\left(5, 4, 3, 2, 1\right)^{\prime}$.
	\item $\hat{\beta}$ ajustados: $\left( 4.840, 4.097, 3.201, 1.880,  1.025 \right)^{\prime}$.
	\item $\hat{\beta}_{p}$ ajustados y con perturbación: $\left( 4.936, 3.966, 3.037,  1.957, 1.081 \right)^{\prime}$.
	\item $\hat{\beta}_{c}$ ajustados con mínimos cuadrados: $\left(4.992, 3.998, 2.961, 1.986, 0.997 \right)^{\prime}$.
\end{itemize}

El experimento se repitió $1000$ veces y en cada una de estas, se midieron las normas: $\left\| \beta - \hat{\beta} \right\|_2$, $\left\| \beta - \hat{\beta}_p \right\|_2$ y $\left\| \beta - \hat{\beta}_c \right\|_2$ para revisar el error y finalmente tomar el promedio, resultando en:
\begin{itemize}
	\item $\beta$ vs $\hat{\beta}$ : $0.19003$.
	\item $\beta$ vs $\hat{\beta}_p$: $0.19482$.
	\item $\beta$ vs $\hat{\beta}_c$: $0.11831$.
\end{itemize}
Por lo que el que tuvo mejor rendimiento, fue el estimador $\hat{\beta}_c = \left(\tilde{X}^{\prime}\tilde{X}\right)^{-1}\tilde{X}^{\prime} y$.

{\color{blue}
	\begin{enumerate}[label=(\alph*), start=2]
		\item  
\end{enumerate}}

Se generó una matriz $X$ mal condicionada (con casi colinealidad). La forma en que se generó fue: se creó un vector aleatorio de tamaño $(n=20)\times 1$, el cual será la primera columna de $X$. El resto de las columnas se generó a raíz de la primera, se le sumó un múltiplo muy pequeño de otro vector aleatorio. Con esto, se obtuvo la matriz $X$ con casi colinealidad en sus columnas y con número de condición $\kappa_{2} = 532,004.6$

Se repitió todo el análisis del inciso anterior (las $1000$ simulaciones) y se obtuvieron los resultados para la última realización:
\begin{itemize}
	\item $\beta$ del modelo original: $\left(5, 4, 3, 2, 1\right)^{\prime}$.
	\item $\hat{\beta}$ ajustados: $\left( -905.614,   129.147, -3831.882,  1039.486,
	3583.851 \right)^{\prime}$.
	\item $\hat{\beta}_{p}$ ajustados y con perturbación: $\left( 0.816, 1.463,  8.534,  3.564, 0.679 \right)^{\prime}$.
	\item $\hat{\beta}_{c}$ ajustados con mínimos cuadrados: $\left( 2.573, 2.929, 3.809, 2.211, 3.458 \right)^{\prime}$.
\end{itemize}
Promediando las normas de interés, se obtuvo que:
\begin{itemize}
	\item $\beta$ vs $\hat{\beta}$ : $7164.56341$.
	\item $\beta$ vs $\hat{\beta}_p$: $5.89879$.
	\item $\beta$ vs $\hat{\beta}_c$: $4.55228$.
\end{itemize}
Notamos que, luego de $1000$ iteraciones, el hecho de que $X$ esté mal condicionada afectó de desempeño de los estimadores. En particular, el de peor error fue $\hat{\beta}$ y el ''mejor'' fue, de nuevo, $\hat{\beta}_c = \left(\tilde{X}^{\prime}\tilde{X}\right)^{-1}\tilde{X}^{\prime} y$.








